<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>João Victor Risso's Blog - GSoC</title><link href="https://joaovictortr.me/" rel="alternate"></link><link href="https://joaovictortr.me/feeds/gsoc.atom.xml" rel="self"></link><id>https://joaovictortr.me/</id><updated>2017-08-29T00:58:00-03:00</updated><entry><title>GSoC - Final Report</title><link href="https://joaovictortr.me/2017/gsoc-final-report.html" rel="alternate"></link><published>2017-08-29T00:58:00-03:00</published><updated>2017-08-29T00:58:00-03:00</updated><author><name>João Victor Risso</name></author><id>tag:joaovictortr.me,2017-08-29:/2017/gsoc-final-report.html</id><summary type="html">&lt;p&gt;Final report of my project on the Google Summer of&amp;nbsp;Code&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Initial&amp;nbsp;Proposal&lt;/h2&gt;
&lt;p&gt;In the initial proposal [4], the goal was to implement wrappers for functions that can be executed on the &lt;span class="caps"&gt;GPU&lt;/span&gt;, as to accelerate computations of models in Theano. More specifically, the goal was to implement the following&amp;nbsp;functionalities:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Wrapper for the warp-ctc library into Theano&lt;/strong&gt;, in order to provide fast &lt;span class="caps"&gt;CTC&lt;/span&gt; computations both in the &lt;span class="caps"&gt;CPU&lt;/span&gt; and &lt;span class="caps"&gt;GPU&lt;/span&gt;. There were two existing wrappers in GitHub, however they were neither complete, or compatible with Theano&amp;#8217;s &lt;code&gt;gpuarray&lt;/code&gt; library.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Wrapper for a Symmetric Eigenvalue Solver&lt;/strong&gt;, using the cuSolver library, in order to obtain the eigenvalues on the &lt;span class="caps"&gt;GPU&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Wrapper for a &lt;span class="caps"&gt;QR&lt;/span&gt; factorization function&lt;/strong&gt;, also using the cuSolver library, which would enable fast eingenvalue computations and&amp;nbsp;factorizations.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Implement Spatial Transformer Network Ops from cuDNN&lt;/strong&gt;, which allows neural networks to handle distorted inputs, and learn how the transformation parameters to better extract features from&amp;nbsp;images.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Changes to the&amp;nbsp;Proposal&lt;/h3&gt;
&lt;p&gt;However, the Theano developers have been working on the implementation of wrappers for functions of the &lt;a href="http://icl.cs.utk.edu/magma/"&gt;&lt;span class="caps"&gt;MAGMA&lt;/span&gt; library&lt;/a&gt;, which implements linear algebra functions in a very efficient manner, with support for multi-cores and GPUs. In the &lt;a href="https://github.com/Theano/Theano/issues/5911"&gt;issue&lt;/a&gt; that lists which operations have been implemented, there are both the items 2 and 3, that is, the &lt;span class="caps"&gt;QR&lt;/span&gt; factorization and eingenvalue solver for symmetric (hermitian)&amp;nbsp;matrices.&lt;/p&gt;
&lt;p&gt;In order to avoid doing rework, it was discussed and decided with mentors that implementing the spatial transformer would be the replacement for items 2 and 3 of the proposal, since it would also be interesting to have a &lt;span class="caps"&gt;CPU&lt;/span&gt; implementation of that&amp;nbsp;functionality.&lt;/p&gt;
&lt;p&gt;Hence, the project was divided into three parts: implementation of the &lt;span class="caps"&gt;CTC&lt;/span&gt; wrapper, spatial transformer using cuDNN, and the &lt;span class="caps"&gt;CPU&lt;/span&gt; spatial&amp;nbsp;transformer.&lt;/p&gt;
&lt;h2&gt;Contributions&lt;/h2&gt;
&lt;p&gt;In this section, I will describe the contributions made to Theano during the course of the&amp;nbsp;project.&lt;/p&gt;
&lt;h3&gt;Connectionist Temporal Classification&amp;nbsp;Loss&lt;/h3&gt;
&lt;p&gt;The first part of the project consisted in implementing a wrapper for Theano that makes use of &lt;a href="https://github.com/baidu-research/warp-ctc"&gt;warp-ctc&lt;/a&gt; [1], a fast implementation of the &lt;span class="caps"&gt;CTC&lt;/span&gt; loss function by Baidu Research. Their implementation works both on multi-core processors (by using OpenMP threads) and also on GPUs, using &lt;span class="caps"&gt;CUDA&lt;/span&gt; kernels to compute the &lt;span class="caps"&gt;CTC&lt;/span&gt; function. A more detailed explanation of how warp-ctc works, is provided in the paper that accompanied the release&amp;nbsp;[2].&lt;/p&gt;
&lt;p&gt;Outputs of a &lt;span class="caps"&gt;CTC&lt;/span&gt; network are given by a softmax layer, whose results are interpreted as a probability distribution over all possible label sequences, conditioned by a given input sequence. Given that distribution, an objective function was derived to maximize the probabilities of correct labellings. Since the objective function is differentiable, the network can be trained with backpropagation through&amp;nbsp;time.&lt;/p&gt;
&lt;p&gt;Implementations of the &lt;span class="caps"&gt;CTC&lt;/span&gt; functionality for &lt;span class="caps"&gt;CPU&lt;/span&gt; and &lt;span class="caps"&gt;GPU&lt;/span&gt;, can be found in Theano&amp;#8217;s &lt;code&gt;theano.tensor.nnet.ctc&lt;/code&gt; and &lt;code&gt;theano.gpuarray.ctc&lt;/code&gt; modules, respectively. Furthermore, optimizations were implemented to allow the user to call a single &lt;span class="caps"&gt;CPU&lt;/span&gt; function, and may have it &amp;#8216;lifted&amp;#8217; for execution on the &lt;span class="caps"&gt;GPU&lt;/span&gt;, depending on his configurations. Finally, wrappers for &lt;span class="caps"&gt;CTC&lt;/span&gt; gradients using were also implemented for both &lt;span class="caps"&gt;CPU&lt;/span&gt; and &lt;span class="caps"&gt;GPU&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Below there is a brief description of each Op, and links to where they are located in Theano&amp;#8217;s&amp;nbsp;codebase:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/Theano/Theano/blob/master/theano/tensor/nnet/ctc.py#L199"&gt;theano.tensor.nnet.ctc.ctc&lt;/a&gt;: function that setups a &lt;span class="caps"&gt;CTC&lt;/span&gt; Op, i.e. it setups the node on the graph that will compute the &lt;span class="caps"&gt;CTC&lt;/span&gt; loss&amp;nbsp;function.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Theano/Theano/blob/master/theano/tensor/nnet/ctc.py#L88"&gt;theano.tensor.nnet.ctc.ConnectionistTemporalClassification&lt;/a&gt;: COp class that implements the computation of the &lt;span class="caps"&gt;CTC&lt;/span&gt; loss&amp;nbsp;function.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Theano/Theano/blob/master/theano/gpuarray/ctc.py#L144"&gt;theano.gpuarray.gpu_ctc&lt;/a&gt;: function that setups a &lt;span class="caps"&gt;GPU&lt;/span&gt; &lt;span class="caps"&gt;CTC&lt;/span&gt; Op, i.e. it setups the node on the graph that will compute the &lt;span class="caps"&gt;CTC&lt;/span&gt; loss function on the &lt;span class="caps"&gt;GPU&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Theano/Theano/blob/master/theano/gpuarray/ctc.py#L20"&gt;theano.gpuarray.GpuConnectionistTemporalClassification&lt;/a&gt;: COp class that implements the computation of the &lt;span class="caps"&gt;CTC&lt;/span&gt; loss function on the &lt;span class="caps"&gt;GPU&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the COp classes, one can find the paths to the C wrappers, which make the interface between Theano and the warp-ctc&amp;nbsp;library.&lt;/p&gt;
&lt;h3&gt;Spatial&amp;nbsp;Transformer&lt;/h3&gt;
&lt;p&gt;In the second and third parts of the project, I have worked on implementing a spatial transformer, at first only on the &lt;span class="caps"&gt;GPU&lt;/span&gt;, and then on the &lt;span class="caps"&gt;CPU&lt;/span&gt;. A Spatial Transformer is a component of a neural network that can provide spatial manipulation of data within the network. Spatial manipulation can improve models by introducing invariance to affine transformations, such as translation, scaling and rotation. This kind of invariance improves classification performance, since the networks become able to recognize samples that have distortions or are rotated, for&amp;nbsp;example.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Spatial Transformer Representation" src="spatial_transformer.png" title="Spatial Transformer Representation"&gt;&lt;/p&gt;
&lt;p&gt;There are three main components in a Spatial Transformer, as shown in the image above (provided in the paper by &lt;a href="https://arxiv.org/abs/1506.02025"&gt;Jaderberg et. al&lt;/a&gt;):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Localisation network:&lt;/strong&gt; neural network that receives the input feature map U, where U is a space spanned by the width, height and channels. It outputs the parameters of the transformation to be applied to the feature map. In 2D, the parameters take the form of a 2x3 matrix (i.e. an affine transformation matrix). It can take the form of any neural network, but it should include a final regression layer to produce the transformation&amp;nbsp;parameters.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Grid generator:&lt;/strong&gt; normalized grid of coordinates over the input feature map. It maps the original coordinate system of the input to an interval in [-1, 1], and applies the transformation the normalized&amp;nbsp;space.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sampler:&lt;/strong&gt; the sampler takes a set of sampling points from the grid generator, along with the input feature map U and produces the sampled output feature map&amp;nbsp;V.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Spatial Transformer using&amp;nbsp;cuDNN&lt;/h4&gt;
&lt;p&gt;cuDNN provides spatial transformer functions since version 6, and those functions were utilized to implement the second part of the project. There are two types of functions: forward and backward. Forward functions implement the operations of the sampling grid, and the sampler. Backward functions are used to compute gradients of the outputs of each forward function, i.e. there is no function to compute gradients of the inputs and another to compute the gradients of the affine transformation, such that they backpropagated in the neural network, in order for it to&amp;nbsp;learn.&lt;/p&gt;
&lt;p&gt;Spatial transformer functions from cuDNN were implemented in Theano&amp;#8217;s &lt;code&gt;gpuarray.dnn&lt;/code&gt; module, as &lt;a href="http://deeplearning.net/software/theano/extending/op.html"&gt;Theano Ops&lt;/a&gt;. In order to wrap the required functions, I have implemented wrappers in C, which interface with Theano &lt;code&gt;PyGpuArrayObject&lt;/code&gt;&lt;span class="quo"&gt;&amp;#8216;&lt;/span&gt;s and the cuDNN&amp;nbsp;functions.&lt;/p&gt;
&lt;p&gt;Below there is a brief description of each Op, and links to where they are located in Theano&amp;#8217;s&amp;nbsp;codebase:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/Theano/Theano/blob/master/theano/gpuarray/dnn.py#L2973"&gt;&lt;code&gt;theano.gpuarray.dnn.dnn_spatialtf&lt;/code&gt;&lt;/a&gt;: function that setups a complete spatial transformer, i.e. it setups the sampling grid and the sampler, and returns the latter to the&amp;nbsp;user.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Theano/Theano/blob/master/theano/gpuarray/dnn.py#L2792"&gt;&lt;code&gt;theano.gpuarray.dnn.GpuDnnTransformerGrid&lt;/code&gt;&lt;/a&gt;: COp class that implements the sampling grid, using cuDNN&amp;#8217;s forward grid generator&amp;nbsp;function.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Theano/Theano/blob/master/theano/gpuarray/dnn.py#L2848"&gt;&lt;code&gt;theano.gpuarray.dnn.GpuDnnTransformerSampler&lt;/code&gt;&lt;/a&gt;: COp class that implements the sampler, which is currently limited by cuDNN to bilinear interpolation. This class interfaces with cuDNN&amp;#8217;s forward sampler&amp;nbsp;function.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Theano/Theano/blob/master/theano/gpuarray/dnn.py#L2907"&gt;&lt;code&gt;theano.gpuarray.dnn.GpuDnnTransformerGradI&lt;/code&gt;&lt;/a&gt;: COp class that implements the gradients of the inputs, which interfaces with cuDNN&amp;#8217;s backward sampling grid&amp;nbsp;function.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Theano/Theano/blob/master/theano/gpuarray/dnn.py#L2944"&gt;&lt;code&gt;theano.gpuarray.dnn.GpuDnnTransformerGradT&lt;/code&gt;&lt;/a&gt;: COp class that implements the gradients of the affine transformation, which interfaces with cuDNN&amp;#8217;s backward sampler&amp;nbsp;function.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the COp classes, one can find the paths to the C wrappers, which make the interface between Theano and&amp;nbsp;cuDNN.&lt;/p&gt;
&lt;h4&gt;Spatial Transformer on the &lt;span class="caps"&gt;CPU&lt;/span&gt;&lt;/h4&gt;
&lt;p&gt;Based on a implementation in &lt;a href="https://github.com/Lasagne/Lasagne/blob/master/lasagne/layers/special.py#L354"&gt;Lasagne&lt;/a&gt;, a spatial transformer was implemented on the &lt;span class="caps"&gt;CPU&lt;/span&gt; as well. The work on this third of the project consisted in adapting the implementation from Lasagne, which uses Theano symbolic variables to perform the computations, into Theano&amp;nbsp;Ops.&lt;/p&gt;
&lt;p&gt;However, Lasagne does not provide the implementations for the gradients, and neither does cuDNN. So those have to be implemented based on the equations of the paper by Jaderberg et al. [3]. Furthemore, it is necessary to provide a concrete implementation (e.g. using NumPy) of each of those Ops, in order to enable users to debug code that uses the functionalities provided by the spatial&amp;nbsp;transformer.&lt;/p&gt;
&lt;p&gt;Most of the implementation is completed, including the gradients of inputs, with only the gradients of the affine transformation currently not passing the gradient tests. Fixing the computations of the affine transformation is the last step required to finish the implementation of the spatial transformer on the &lt;span class="caps"&gt;CPU&lt;/span&gt;.&lt;/p&gt;
&lt;h3&gt;Pull&amp;nbsp;Requests&lt;/h3&gt;
&lt;p&gt;A vast majority of the discussions with the mentors during the course of the project, were carried out in the public mailing list of Theano developers, and on GitHub Pull Requests. Furthermore, all implementations passed through a process of unit testing, and peer review by the&amp;nbsp;mentors.&lt;/p&gt;
&lt;p&gt;First and second parts of the project were successfully completed, and are already merged into Theano (see the Pull Requests below). However, the third part is not yet completed, for reasons that have already been&amp;nbsp;explained.&lt;/p&gt;
&lt;p&gt;Links to the original Pull Requests are provided&amp;nbsp;below:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Connectionist Temporal Classification Loss with warp-ctc: &lt;a href="https://github.com/Theano/Theano/pull/5949"&gt;Pull&amp;nbsp;Request&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Spatial Transformer using cuDNN: &lt;a href="https://github.com/Theano/Theano/pull/6061"&gt;Pull&amp;nbsp;Request&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Spatial Transformer on the &lt;span class="caps"&gt;CPU&lt;/span&gt; (&lt;span class="caps"&gt;WIP&lt;/span&gt;): &lt;a href="https://github.com/Theano/Theano/pull/6298"&gt;Pull&amp;nbsp;Request&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;You can also see my commits in Theano, &lt;a href="https://github.com/Theano/Theano/commits/master?author=joaovictortr"&gt;here&lt;/a&gt; for the &lt;span class="caps"&gt;CTC&lt;/span&gt; and Spatial Transformer with cuDNN, and &lt;a href="https://github.com/joaovictortr/Theano/commits/spatialtf_cpu?author=joaovictortr"&gt;here&lt;/a&gt; for the Spatial Transformer on the &lt;span class="caps"&gt;CPU&lt;/span&gt;.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this project, I have implemented wrappers for &lt;span class="caps"&gt;GPU&lt;/span&gt; functions in Theano, in order to accelerate the computation of deep learning models. Two of the three parts of the project have been merged into Theano, with the third only requiring fixing the computation of&amp;nbsp;gradients.&lt;/p&gt;
&lt;p&gt;During this summer, I have learned a lot about the inner workings of Theano. I have also improved considerably my knowledge of Python, as I come from a strong C/C++&amp;nbsp;background.&lt;/p&gt;
&lt;h3&gt;What&amp;#8217;s&amp;nbsp;Next?&lt;/h3&gt;
&lt;p&gt;I&amp;#8217;ll start getting deeper into machine learning, and Theano will be a great tool for the job. With some knowledge of the internals, I can implement my own models, as well as suggest and add new&amp;nbsp;functionalities.&lt;/p&gt;
&lt;h2&gt;Acknowledgements&lt;/h2&gt;
&lt;p&gt;I would like to thank Steven Bocco, my mentor, for guiding me in the execution of the project, providing feedback and reviewing the code. I would also like to thank Frédéric Bastien and Arnaud Bergeron, for helping with organizational aspects, and code&amp;nbsp;reviewing.&lt;/p&gt;
&lt;p&gt;Finally, I would like to thank the staff of GSoC, and Google, for this unique&amp;nbsp;opportunity.&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;[1] &lt;a href="https://insidehpc.com/2016/01/warp-ctc/"&gt;&amp;#8220;Accelerating Machine Learning with Open Source Warp-&lt;span class="caps"&gt;CTC&lt;/span&gt;&amp;#8221;&lt;/a&gt;. 2016. Accessed on:&amp;nbsp;2017-08-28.&lt;/p&gt;
&lt;p&gt;[2] Amodei et. al. &lt;a href="https://arxiv.org/abs/1512.02595"&gt;Deep Speech 2: End-to-End Speech Recognition in English and Mandarin&lt;/a&gt;. 2015. Accessed on:&amp;nbsp;2017-08-28&lt;/p&gt;
&lt;p&gt;[3] Jaderberg et. al. &lt;a href="https://arxiv.org/abs/1506.02025"&gt;Spatial Transformer Networks&lt;/a&gt;. 2015. Accessed on:&amp;nbsp;2017-08-28&lt;/p&gt;
&lt;p&gt;[4] Risso, &lt;span class="caps"&gt;J. V.&lt;/span&gt; T.. &lt;a href="https://www.sharelatex.com/project/58cee4bcf98f21f60fedec74"&gt;Extend usage of optimized &lt;span class="caps"&gt;GPU&lt;/span&gt; libraries in Theano&lt;/a&gt;. 2017. Accessed on:&amp;nbsp;2017-08-28&lt;/p&gt;</content><category term="python"></category><category term="gsoc"></category></entry><entry><title>GSoC - Spatial Transformer 3</title><link href="https://joaovictortr.me/2017/gsoc-spatial-transformer3.html" rel="alternate"></link><published>2017-08-06T16:04:00-03:00</published><updated>2017-08-06T16:04:00-03:00</updated><author><name>João Victor Risso</name></author><id>tag:joaovictortr.me,2017-08-06:/2017/gsoc-spatial-transformer3.html</id><summary type="html">&lt;p&gt;In this post, I&amp;#8217;ll present a third update on the Spatial Transformer development in&amp;nbsp;Theano&lt;/p&gt;</summary><content type="html">&lt;p&gt;Spatial transformer implementation in the &lt;span class="caps"&gt;GPU&lt;/span&gt; using cuDNN is now complete. The
problem with the gradient computation was solved, with the help of my mentor.
Also, comprehensive unit tests of the Op and the gradients were added. Further
details can be found in the units tests, and discussion in the &lt;a href="https://github.com/Theano/Theano/pull/6061"&gt;pull request&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As future direction, and last part of my GSoC project, I will implement a &lt;span class="caps"&gt;CPU&lt;/span&gt;
version of the spatial transformer, using symbolic operations from Theano. The
initial implementation and discussions can be followed on the appropriate &lt;a href="https://github.com/Theano/Theano/pull/6298"&gt;pull request&lt;/a&gt; as&amp;nbsp;well.&lt;/p&gt;</content><category term="python"></category><category term="gsoc"></category></entry><entry><title>GSoC - Spatial Transformer 2</title><link href="https://joaovictortr.me/2017/gsoc-spatial-transformer2.html" rel="alternate"></link><published>2017-07-24T14:37:00-03:00</published><updated>2017-07-24T14:37:00-03:00</updated><author><name>João Victor Risso</name></author><id>tag:joaovictortr.me,2017-07-24:/2017/gsoc-spatial-transformer2.html</id><summary type="html">&lt;p&gt;In this post, I&amp;#8217;ll present an update on the Spatial Transformer development in&amp;nbsp;Theano&lt;/p&gt;</summary><content type="html">&lt;p&gt;Spatial transformer implementation is still not yet completed, due to some
issues in the implementation of the gradients. These issues should be solved
by this week, and a &lt;span class="caps"&gt;CPU&lt;/span&gt; implementation of the spatial transformer might also
be implemented thereafter, which is already implemented in Lasagne, but needs
to be ported into&amp;nbsp;Theano.&lt;/p&gt;
&lt;p&gt;Most of the problem with gradients consists in some &lt;code&gt;DisconnectedType&lt;/code&gt; objects
appearing in the computational graph, where Theano variables would be expected,
when we want to compute and verify the gradients in the unit tests. I have tried
several different approaches to setup the grad test, but have not succeeded in
any of&amp;nbsp;them.&lt;/p&gt;
&lt;p&gt;Gradient computations in Theano implement, through symbolic operations, the chain
rule of differential&amp;nbsp;calculus:&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\frac{\partial C}{\partial x} = \frac{\partial C}{\partial f} * \frac{\partial f}{\partial&amp;nbsp;x}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Where C is the cost function that returns a scalar, f is a function computed by
the spatial transformer over a vector or tensor x, and x consists in our input images.
In order to compute the gradient of the inputs with respect to the cost function, one
must implement the chain rule using symbolic operations. However, in the case of the
gradients of the spatial transformer, the issue is that the derivative of the cost
with respect to f is a &lt;code&gt;DisconnectedType&lt;/code&gt; object, i.e. not a Theano variable, as
one would&amp;nbsp;expect.&lt;/p&gt;
&lt;p&gt;In order summarize the changes, here is a short list of what as changed since the last&amp;nbsp;time:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Op now uses scaling factors to set the dimensions of the sampling&amp;nbsp;grid.&lt;/li&gt;
&lt;li&gt;Op implementation was merged into a single class&amp;nbsp;(GpuDnnTransformer).&lt;/li&gt;
&lt;li&gt;Initial implementations of the gradients for both inputs and affine transformation were&amp;nbsp;added.&lt;/li&gt;
&lt;li&gt;Comprehensive tests and type checks were added to verify the Op&amp;nbsp;implementation.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You can follow the &lt;a href="https://github.com/Theano/Theano/pull/6061"&gt;Pull Request&lt;/a&gt; to
keep track of the ongoing development and&amp;nbsp;discussions.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="python"></category><category term="gsoc"></category></entry><entry><title>GSoC - Spatial Transformer</title><link href="https://joaovictortr.me/2017/gsoc-spatial-transformer.html" rel="alternate"></link><published>2017-06-29T00:30:00-03:00</published><updated>2017-06-29T00:30:00-03:00</updated><author><name>João Victor Risso</name></author><id>tag:joaovictortr.me,2017-06-29:/2017/gsoc-spatial-transformer.html</id><summary type="html">&lt;p&gt;In this post, I&amp;#8217;ll present a follow up on the Spatial Transformer development in&amp;nbsp;Theano&lt;/p&gt;</summary><content type="html">&lt;p&gt;Currently, I am working on the implementation of a wrapper over cuDNN functions
to provide a Spatial Transformer in Theano. Today, I&amp;#8217;ve got the first part of
the transformer working. In this post, I&amp;#8217;ll show a basic example of how to use
this initial&amp;nbsp;functionality.&lt;/p&gt;
&lt;p&gt;First, let&amp;#8217;s obtain an image onto which we will apply some transformations. There
is simple example image provided by scipy. Then, we&amp;#8217;ll import scipy, matplotlib
(to view the image), and then load the&amp;nbsp;image:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;scipy&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;misc&lt;/span&gt;
&lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;misc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;face&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imshow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Which should show the following&amp;nbsp;image:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Raccoon face image from scipy.misc module" src="https://joaovictortr.me/images/raccoon_face.png"&gt;&lt;/p&gt;
&lt;p&gt;Images are usually represented with (h, w, c) data layout, where h is the
height, w is the width, and c is the number of color channels. Also, the pixel
values are represented in a range of integer values in [0, 255]. When we pack
a set of images with the data layout in a single Numpy array, we get a data
layout of (n, h, w, c), where n is the number of images, becoming a 4-dimensional
array (or a 4-D&amp;nbsp;tensor).&lt;/p&gt;
&lt;p&gt;Since the spatial transformer utilizes a grid to sample data from the input, we
must define its dimensions. If the width and height dimensions are bigger than
the corresponding ones from the input image, we will perform oversampling, using
bilinear interpolation. Conversely, by using a smaller width or height, we will
perform an subsampling of the original image, also with bilinear&amp;nbsp;interpolation.&lt;/p&gt;
&lt;p&gt;So, let&amp;#8217;s define our grid dimensions as&amp;nbsp;follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# shape: (num_images, channels, height, width)&lt;/span&gt;
&lt;span class="n"&gt;grid_dims&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;128&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;128&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The layout of the grid dimensions is given by the number of images, followed by
the number of channels, height and width of these&amp;nbsp;images.&lt;/p&gt;
&lt;p&gt;Packing images together is relevant in this case, since the spatial transformer
functions from cuDNN expect a 4D data layout, we pack our original image in the
(n, h, w, c), while adding more examples of the same image to the final&amp;nbsp;array:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;asarray&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;grid_dims&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Also, our input images need to be normalized into the interval [-1, 1] (see the
&lt;a href="https://arxiv.org/abs/1506.02025"&gt;original paper&lt;/a&gt; for more details). So we convert
our images to float, then normalize&amp;nbsp;them:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;normalize_input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="c1"&gt;# Scale input from [0, 255] to [0, 2]&lt;/span&gt;
    &lt;span class="n"&gt;scale_factor&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;  &lt;span class="c1"&gt;# equivalent to 1 / 128&lt;/span&gt;
    &lt;span class="nb"&gt;input&lt;/span&gt; &lt;span class="o"&gt;*=&lt;/span&gt; &lt;span class="n"&gt;scale_factor&lt;/span&gt;
    &lt;span class="c1"&gt;# Re-scale input from [0, 2] to [-1, 1] (normalized)&lt;/span&gt;
    &lt;span class="nb"&gt;input&lt;/span&gt; &lt;span class="o"&gt;-=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nb"&gt;input&lt;/span&gt;

&lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;normalize_input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;astype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The &lt;code&gt;normalize_input&lt;/code&gt; function takes the images as input, and normalizes the
pixel values into the range [-1, 1]. Multiplication and subtraction work on
each element of the array through &lt;a href="https://docs.scipy.org/doc/numpy-1.12.0/user/basics.broadcasting.html"&gt;broadcasting&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Now, we have to obtain the transformation we want to apply on the image. The
transformation is usually obtained from a localisation network, which must
have a regression layer at the output layer. A localisation network can be
built on top of already existing components of&amp;nbsp;Theano.&lt;/p&gt;
&lt;p&gt;In this case, we&amp;#8217;ll use a predefined transformation, to show how the spatial
transformer works without the localisation net. Suppose we want to flip the
image - rotate it by 180 degrees, in order to achieve that using an affine
transformation, we have to define a rotation operation on our transformation&amp;nbsp;matrix:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Rotation matrix for 180 degree rotation, including translation&lt;/span&gt;
&lt;span class="n"&gt;rotate&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
          &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;
&lt;span class="c1"&gt;# One matrix is applied for each image, in this case we use the same matrix&lt;/span&gt;
&lt;span class="c1"&gt;# for all images.&lt;/span&gt;
&lt;span class="n"&gt;theta&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;asarray&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;grid_dims&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;rotate&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Spatial transformer also expects the data layout of the tensor to be in (n, c, h, w),
instead of the (n, h, w, c) we have created, so we have to transpose the&amp;nbsp;tensor:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transpose&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axes&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We could also perform the transpose operation on a Theano symbolic variable,
using the transpose function of &lt;code&gt;theano.tensor&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Now we are ready to transform the images, and then we instantiate the spatial&amp;nbsp;transformer:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;theano.gpuarray.dnn&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;dnn_spatialtf&lt;/span&gt;
&lt;span class="n"&gt;transformer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dnn_spatialf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;theta&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;grid_dims&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Then, we have to compile and call a theano function to compute the transformed&amp;nbsp;values:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Create theano function to compute transformation&lt;/span&gt;
&lt;span class="n"&gt;fn&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;theano&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;function&lt;/span&gt;&lt;span class="p"&gt;([],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;transformer&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="c1"&gt;# Compute transformation&lt;/span&gt;
&lt;span class="n"&gt;out_img_gpu&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;fn&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now, we have three things to&amp;nbsp;consider:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Results are in the &lt;span class="caps"&gt;GPU&lt;/span&gt; memory, so we have to copy them&amp;nbsp;back&lt;/li&gt;
&lt;li&gt;Resulting values of the images are in the [-1, 1] range, so we have to rescale them to [0,&amp;nbsp;255]&lt;/li&gt;
&lt;li&gt;Data layout of resulting images is (n, c, h, w), so we have to convert them to (n, h, w, c), in order to visualize the&amp;nbsp;images.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Which we will do&amp;nbsp;next:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;rescale_input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="c1"&gt;# Re-scale output to range [0, 2]&lt;/span&gt;
    &lt;span class="nb"&gt;input&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="c1"&gt;# Re-scale output to range [0, 255]&lt;/span&gt;
    &lt;span class="nb"&gt;input&lt;/span&gt; &lt;span class="o"&gt;*=&lt;/span&gt; &lt;span class="mi"&gt;128&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nb"&gt;input&lt;/span&gt;

&lt;span class="c1"&gt;# Copy results back from the GPU&lt;/span&gt;
&lt;span class="n"&gt;out_img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;asarray&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;out_img_gpu&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# Re-scale values from [-1, 1] to [0, 255], and convert to uint8&lt;/span&gt;
&lt;span class="n"&gt;out_img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;rescale_input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;out_img&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;astype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;uint8&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# Convert from NCHW to NHWC&lt;/span&gt;
&lt;span class="n"&gt;out_img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transpose&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;out_img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axes&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Finally, we can visualize our transformed&amp;nbsp;images:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;img_idx&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;out_img&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imshow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;out_img&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;img_idx&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Which would look like the&amp;nbsp;following:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Raccoon face image from scipy.misc module" src="https://joaovictortr.me/images/raccoon_face_flipped.png"&gt;&lt;/p&gt;
&lt;p&gt;In this case, the image is subsampled, because our grid dimensions are smaller
than those of the original image. However, we can still see the original image&amp;nbsp;flipped.&lt;/p&gt;
&lt;p&gt;That is it for today, however there are tasks left to complete the&amp;nbsp;implementation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Add proper tests to the already implemented&amp;nbsp;functionality&lt;/li&gt;
&lt;li&gt;Gradients are not yet supported, so one of the next steps is implementing the
backward operations to implement the gradients (allowing&amp;nbsp;backpropagation)&lt;/li&gt;
&lt;li&gt;Add proper tests also to the&amp;nbsp;gradients&lt;/li&gt;
&lt;li&gt;Provide a functional example: I&amp;#8217;m working with a &lt;a href="https://github.com/Lasagne/Recipes/blob/master/examples/spatial_transformer_network.ipynb"&gt;Lasagne implementation&lt;/a&gt;
to build a neural net model that can be used with the &lt;span class="caps"&gt;GPU&lt;/span&gt; Spatial&amp;nbsp;Transformer.&lt;/li&gt;
&lt;/ul&gt;</content><category term="python"></category><category term="gsoc"></category></entry><entry><title>GSoC - The Road So Far</title><link href="https://joaovictortr.me/2017/gsoc-project-progress-report.html" rel="alternate"></link><published>2017-06-26T22:30:00-03:00</published><updated>2017-06-26T22:30:00-03:00</updated><author><name>João Victor Risso</name></author><id>tag:joaovictortr.me,2017-06-26:/2017/gsoc-project-progress-report.html</id><summary type="html">&lt;p&gt;In this post, I will talk about the progress of my GSoC project from Jun, 11 until Jun,&amp;nbsp;26.&lt;/p&gt;</summary><content type="html">&lt;p&gt;In the last post, I have introduced what the GSoC project consisted of and the
operations I intend to implement by the end of the program. I am happy to announce
that the first part of the project is now completed, however it is still
pending to be merged. You can find the &lt;span class="caps"&gt;PR&lt;/span&gt; with the code and discussions on
the wrapper implementation &lt;a href="https://github.com/Theano/Theano/pull/5949"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Connectionist Temporal&amp;nbsp;Classification&lt;/h3&gt;
&lt;p&gt;The first part of the project consisted in implementing a wrapper for Theano
that makes use of &lt;a href="https://github.com/baidu-research/warp-ctc"&gt;warp-ctc&lt;/a&gt;,
a fast implementation of the &lt;span class="caps"&gt;CTC&lt;/span&gt; loss function by Baidu Research. Their
implementation works both on multi-core processors (by using OpenMP threads)
and also on GPUs, using &lt;span class="caps"&gt;CUDA&lt;/span&gt; kernels to compute the &lt;span class="caps"&gt;CTC&lt;/span&gt; function. More details
on the inner workings of warp-ctc, can be found
&lt;a href="https://github.com/baidu-research/warp-ctc"&gt;here&lt;/a&gt; and &lt;a href="http://arxiv.org/abs/1512.02595"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class="caps"&gt;CTC&lt;/span&gt; stands for Connectionist Temporal Classification and it was proposed by
&lt;a href="http://www.cs.toronto.edu/%7Egraves/icml_2006.pdf"&gt;Graves &lt;em&gt;et.al&lt;/em&gt;&lt;/a&gt;. &lt;span class="caps"&gt;CTC&lt;/span&gt;
consists in a loss function that allows temporal classification of unsegmented
data in recurrent neural networks (RNNs). Problems with unsegmented data are
very common in perceptual tasks, such as handwriting recognition and speech&amp;nbsp;recognition.&lt;/p&gt;
&lt;p&gt;Labelling unsegmented sequence data, such as images and sound, was challenging
in RNNs because objective functions for these kinds of networks are defined
separately for each point (of features) in the training sequence. That meant
that training data had to be pre- and post-processed to obtain a final label&amp;nbsp;sequence.&lt;/p&gt;
&lt;p&gt;Outputs of a &lt;span class="caps"&gt;CTC&lt;/span&gt; network are given by a softmax layer, whose results are
interpreted as a probability distribution over all possible label sequences,
conditioned by a given input sequence. Given that distribution, an objective
function was derived to maximize the probabilities of correct labellings. Since
the objective function is differentiable, the network can be trained with
backpropagation through&amp;nbsp;time.&lt;/p&gt;
&lt;p&gt;At the moment, I am working on a functional example of the &lt;span class="caps"&gt;CTC&lt;/span&gt; functionality to
showcase how the implementation can be used in Theano. I&amp;#8217;m building the network
using Lasagne, which is a lightweight library of neural network building
blocks on top of&amp;nbsp;Theano.&lt;/p&gt;
&lt;h3&gt;Spatial&amp;nbsp;Transformer&lt;/h3&gt;
&lt;p&gt;I have started working on the Spatial Transformer Op last week, with some of
the basic functionality already in place. The Op is built on top of cuDNN
functions that provide the components needed for a Spatial Transformer,
therefore the implementation will be limited to GPUs at the&amp;nbsp;moment.&lt;/p&gt;
&lt;p&gt;A Spatial Transformer is a component of a neural network that can provide
spatial manipulation of data within the network. Spatial manipulation can
improve models by introducing invariance to affine transformations, such as
translation, scaling and rotation. This kind of invariance improves classification
performance, since the networks become able to recognize, for example, distorted&amp;nbsp;samples.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Spatial Transformer. Source: original paper by Jaderberg **et. al*" src="https://joaovictortr.me/images/spatial_transformer.png"&gt;&lt;/p&gt;
&lt;p&gt;There are three main components in a Spatial Transformer, as shown in the image
above (provided in the &lt;a href="https://arxiv.org/abs/1506.02025"&gt;paper by Jaderberg &lt;em&gt;et. al&lt;/em&gt;&lt;/a&gt;):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Localisation network: neural network that receives the input feature map U,
where U is a space spanned by the width, height and channels. It outputs the
parameters of the transformation to be applied to the feature map. In 2D, the
parameters take the form of a 2x3 matrix (i.e. an affine transformation matrix).
It can take the form of any neural network, but it should include a final regression
layer to produce the transformation&amp;nbsp;parameters.&lt;/li&gt;
&lt;li&gt;Grid generator: normalized grid of coordinates over the input feature map. It
maps the original coordinate system of the input to an interval in [-1, 1], and
applies the transformation the normalized&amp;nbsp;space.&lt;/li&gt;
&lt;li&gt;Sampler: sampler take a set of sampling points from the grid generator, along
with the input feature map U and produces the sampled output feature map&amp;nbsp;V.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The cuDNN library provides functions to create both the grid generator and the
sampler as well as the back-propagation functions. Therefore, the main task of
the second part is to provide a wrapper in Theano over these functions, in order
to provide the Spatial Transformer Op. There is a limitation however, cuDNN can
only handle 2D transformations on the inputs, so for now mainly images are&amp;nbsp;supported.&lt;/p&gt;
&lt;p&gt;Since it is still a work in progress, further testing is necessary for the grid
generator and the sampler. Back-propagation is not yet implemented, but it will
be soon. You can keep up with the development of the wrapper and provide feedback
in the &lt;a href="https://github.com/Theano/Theano/pull/6061"&gt;pull request&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Showcasing and&amp;nbsp;Examples&lt;/h3&gt;
&lt;p&gt;I have started working today with Lasagne to implement neural network models to
demonstrate how to utilize the implementations, serving as a reference for users
interested in the&amp;nbsp;functionalities.&lt;/p&gt;
&lt;p&gt;So far, I am trying to find an interesting dataset to work with, specially on &lt;span class="caps"&gt;CTC&lt;/span&gt;,
mainly because the lack of open speech datasets. Also, the models proposed in the
original papers are somewhat complex, so they might take too much time to build,
so I will focus on building simpler&amp;nbsp;models.&lt;/p&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;I have finished the first implementation part of the project, by providing the
wrapper warp-ctc. Now, I am working on implementing the Spatial Transformer,
with some of the basic functionalities already in place. Furthermore, I intend
to provide some simple models to demonstrate how to use the functionalities
using Lasagne and&amp;nbsp;Theano.&lt;/p&gt;</content><category term="python"></category><category term="gsoc"></category></entry><entry><title>Starting at GSoC 2017 with Theano</title><link href="https://joaovictortr.me/2017/gsoc-starting-with-theano.html" rel="alternate"></link><published>2017-06-11T23:45:00-03:00</published><updated>2017-06-11T23:45:00-03:00</updated><author><name>João Victor Risso</name></author><id>tag:joaovictortr.me,2017-06-11:/2017/gsoc-starting-with-theano.html</id><summary type="html">&lt;p&gt;I&amp;#8217;ve been accepted for GSoC 2017, in this post I&amp;#8217;ll talk about some details of the&amp;nbsp;project&lt;/p&gt;</summary><content type="html">&lt;p&gt;I have been selected for GSoC 2017 and I&amp;#8217;m glad to work on a project to bring
more &lt;span class="caps"&gt;GPU&lt;/span&gt; acceleration to &lt;a href="https://github.com/Theano/Theano"&gt;Theano&lt;/a&gt;, a sub-org
of the &lt;a href="https://www.python.org/psf/"&gt;Python Software Foundation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This summer I&amp;#8217;ll add support for some &lt;span class="caps"&gt;GPU&lt;/span&gt; libraries in Theano, which will help
applications to run potentially faster in &lt;span class="caps"&gt;GPU&lt;/span&gt;-enabled&amp;nbsp;systems.&lt;/p&gt;
&lt;p&gt;Why &lt;span class="caps"&gt;GPU&lt;/span&gt; acceleration matters,&amp;nbsp;anyway?&lt;/p&gt;
&lt;p&gt;GPUs are becoming increasingly important
to accelerate Deep Learning applications. These accelerators provide massive
parallelism through lots of concurrent threads. Parallel processing of images,
speech and video, for example, speeds up the tasks of training and inference
in neural&amp;nbsp;networks.&lt;/p&gt;
&lt;p&gt;What is&amp;nbsp;Theano?&lt;/p&gt;
&lt;p&gt;Theano is a Python library to handle mathematical expressions
with multi-dimensional arrays (tensors) in an efficient way. Theano has been
developed in the &lt;a href="https://mila.umontreal.ca/"&gt;Machine Institute for Learning Algorithms (&lt;span class="caps"&gt;MILA&lt;/span&gt;)&lt;/a&gt;,
at the University of Montreal, since 2007. It is commonly utilized to implement
neural network models, and provides several features, such&amp;nbsp;as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Efficient symbolic differentiation: the library computes the derivatives of
functions with one or many&amp;nbsp;inputs.&lt;/li&gt;
&lt;li&gt;Speed and stability optimizations: Theano can rearrange your computations,
in order to obtain more numerical stability. It also support dynamic C code
generation, which provides efficient code to perform your&amp;nbsp;computations.&lt;/li&gt;
&lt;li&gt;Transparent use of a &lt;span class="caps"&gt;GPU&lt;/span&gt;: compute-intensive tasks can be performed on a &lt;span class="caps"&gt;GPU&lt;/span&gt;
transparently, and some operations can lift the &lt;span class="caps"&gt;CPU&lt;/span&gt; tasks automatically to
the &lt;span class="caps"&gt;GPU&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As Marek Rei pointed out in a &lt;a href="http://www.marekrei.com/blog/theano-tutorial/"&gt;blog post&lt;/a&gt;,
Theano isn&amp;#8217;t actually a machine learning library, but it provides you with
means to build your own machine learning&amp;nbsp;models.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://summerofcode.withgoogle.com/projects/#5331912125054976"&gt;My project&lt;/a&gt;
for this summer consists of three main&amp;nbsp;tasks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Integrate Baidu Research&amp;#8217;s &lt;a href="https://github.com/baidu-research/warp-ctc"&gt;warp-ctc&lt;/a&gt;
library to provide fast connectionist temporal classification (&lt;span class="caps"&gt;CTC&lt;/span&gt;) loss
computations. Work on this feature is almost completed (for both &lt;span class="caps"&gt;CPU&lt;/span&gt; and &lt;span class="caps"&gt;GPU&lt;/span&gt;),
and should be merged soon. You can find more details on the
&lt;a href="https://github.com/Theano/Theano/pull/5949"&gt;pull request&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Add further &lt;span class="caps"&gt;GPU&lt;/span&gt;-accelerated linear algebra functions to Theano&amp;#8217;s new &lt;span class="caps"&gt;GPU&lt;/span&gt;
backend. Details from the original project have changed, so I&amp;#8217;ll add a follow
up post as soon as&amp;nbsp;possible.&lt;/li&gt;
&lt;li&gt;Implement Spatial Transformer Networks&amp;#8217; operations from the cuDNN library.
These networks are composed of three main components: a localization network,
a grid generator and a grid sampler. You can find more details on the
&lt;a href="https://arxiv.org/abs/1506.02025"&gt;original paper&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I will add more posts as the project progresses throughout the&amp;nbsp;summer.&lt;/p&gt;</content><category term="python"></category><category term="gsoc"></category></entry></feed>