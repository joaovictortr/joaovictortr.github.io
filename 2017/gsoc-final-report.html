<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="João Victor Risso's Blog, ">

        <link rel="alternate"  href="https://joaovictortr.me/feeds/all.atom.xml" type="application/atom+xml" title="João Victor Risso's Blog Full Atom Feed"/>

        <title>GSoC - Final Report // João Victor Risso's Blog // </title>


    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/pure/0.3.0/pure-min.css">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.1.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://joaovictortr.me/theme/css/pure.css">
    <link rel="stylesheet" href="https://joaovictortr.me/theme/css/pygments.css">

    <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/fitvids/1.0.1/jquery.fitvids.min.js"></script>
    <script>
        $(document).ready(function(){
            $(".content").fitVids();
        });
    </script>
</head>

<body>
    <div class="pure-g-r" id="layout">
        <div class="sidebar pure-u">
            <div class="cover-img" style="background: none repeat scroll 0% 0% #3D4F5D">
                <div class="cover-body">
                    <header class="header">
                        <hgroup>
                            <img class="avatar" src="https://joaovictortr.me/images/prof_bild.jpg">
                            <h1 class="brand-main"><a href="https://joaovictortr.me">João Victor Risso's Blog</a></h1>
                            <p class="tagline"></p>
                                <p class="social">
                                    <a href="https://github.com/joaovictortr">
                                        <i class="fa fa-github fa-3x"></i>
                                    </a>
                                    <a href="https://twitter.com/joaovictortr_">
                                        <i class="fa fa-twitter-square fa-3x"></i>
                                    </a>
                                    <a href="https://joaovictortr.me/feeds/all.atom.xml">
                                        <i class="fa fa-rss fa-3x"></i>
                                    </a>
                                </p>
                        </hgroup>
                    </header>
                </div>
            </div>
        </div>
    <div class="pure-u">
        <div class="content">
            <section class="post">
                <header class="post-header">
                    <h1>GSoC - Final&nbsp;Report</h1>
                        <p class="post-meta">
                            // under                                 <a class="post-category" href="https://joaovictortr.me/tag/python.html">python</a>
                                <a class="post-category" href="https://joaovictortr.me/tag/gsoc.html">gsoc</a>
                        </p>
                </header>
            </section>
            <h2>Initial&nbsp;Proposal</h2>
<p>In the initial proposal [4], the goal was to implement wrappers for functions that can be executed on the <span class="caps">GPU</span>, as to accelerate computations of models in Theano. More specifically, the goal was to implement the following&nbsp;functionalities:</p>
<ol>
<li><strong>Wrapper for the warp-ctc library into Theano</strong>, in order to provide fast <span class="caps">CTC</span> computations both in the <span class="caps">CPU</span> and <span class="caps">GPU</span>. There were two existing wrappers in GitHub, however they were neither complete, or compatible with Theano&#8217;s <code>gpuarray</code> library.</li>
<li><strong>Wrapper for a Symmetric Eigenvalue Solver</strong>, using the cuSolver library, in order to obtain the eigenvalues on the <span class="caps">GPU</span>.</li>
<li><strong>Wrapper for a <span class="caps">QR</span> factorization function</strong>, also using the cuSolver library, which would enable fast eingenvalue computations and&nbsp;factorizations.</li>
<li><strong>Implement Spatial Transformer Network Ops from cuDNN</strong>, which allows neural networks to handle distorted inputs, and learn how the transformation parameters to better extract features from&nbsp;images.</li>
</ol>
<h3>Changes to the&nbsp;Proposal</h3>
<p>However, the Theano developers have been working on the implementation of wrappers for functions of the <a href="http://icl.cs.utk.edu/magma/"><span class="caps">MAGMA</span> library</a>, which implements linear algebra functions in a very efficient manner, with support for multi-cores and GPUs. In the <a href="https://github.com/Theano/Theano/issues/5911">issue</a> that lists which operations have been implemented, there are both the items 2 and 3, that is, the <span class="caps">QR</span> factorization and eingenvalue solver for symmetric (hermitian)&nbsp;matrices.</p>
<p>In order to avoid doing rework, it was discussed and decided with mentors that implementing the spatial transformer would be the replacement for items 2 and 3 of the proposal, since it would also be interesting to have a <span class="caps">CPU</span> implementation of that&nbsp;functionality.</p>
<p>Hence, the project was divided into three parts: implementation of the <span class="caps">CTC</span> wrapper, spatial transformer using cuDNN, and the <span class="caps">CPU</span> spatial&nbsp;transformer.</p>
<h2>Contributions</h2>
<p>In this section, I will describe the contributions made to Theano during the course of the&nbsp;project.</p>
<h3>Connectionist Temporal Classification&nbsp;Loss</h3>
<p>The first part of the project consisted in implementing a wrapper for Theano that makes use of <a href="https://github.com/baidu-research/warp-ctc">warp-ctc</a> [1], a fast implementation of the <span class="caps">CTC</span> loss function by Baidu Research. Their implementation works both on multi-core processors (by using OpenMP threads) and also on GPUs, using <span class="caps">CUDA</span> kernels to compute the <span class="caps">CTC</span> function. A more detailed explanation of how warp-ctc works, is provided in the paper that accompanied the release&nbsp;[2].</p>
<p>Outputs of a <span class="caps">CTC</span> network are given by a softmax layer, whose results are interpreted as a probability distribution over all possible label sequences, conditioned by a given input sequence. Given that distribution, an objective function was derived to maximize the probabilities of correct labellings. Since the objective function is differentiable, the network can be trained with backpropagation through&nbsp;time.</p>
<p>Implementations of the <span class="caps">CTC</span> functionality for <span class="caps">CPU</span> and <span class="caps">GPU</span>, can be found in Theano&#8217;s <code>theano.tensor.nnet.ctc</code> and <code>theano.gpuarray.ctc</code> modules, respectively. Furthermore, optimizations were implemented to allow the user to call a single <span class="caps">CPU</span> function, and may have it &#8216;lifted&#8217; for execution on the <span class="caps">GPU</span>, depending on his configurations. Finally, wrappers for <span class="caps">CTC</span> gradients using were also implemented for both <span class="caps">CPU</span> and <span class="caps">GPU</span>.</p>
<p>Below there is a brief description of each Op, and links to where they are located in Theano&#8217;s&nbsp;codebase:</p>
<ul>
<li><a href="https://github.com/Theano/Theano/blob/master/theano/tensor/nnet/ctc.py#L199">theano.tensor.nnet.ctc.ctc</a>: function that setups a <span class="caps">CTC</span> Op, i.e. it setups the node on the graph that will compute the <span class="caps">CTC</span> loss&nbsp;function.</li>
<li><a href="https://github.com/Theano/Theano/blob/master/theano/tensor/nnet/ctc.py#L88">theano.tensor.nnet.ctc.ConnectionistTemporalClassification</a>: COp class that implements the computation of the <span class="caps">CTC</span> loss&nbsp;function.</li>
<li><a href="https://github.com/Theano/Theano/blob/master/theano/gpuarray/ctc.py#L144">theano.gpuarray.gpu_ctc</a>: function that setups a <span class="caps">GPU</span> <span class="caps">CTC</span> Op, i.e. it setups the node on the graph that will compute the <span class="caps">CTC</span> loss function on the <span class="caps">GPU</span>.</li>
<li><a href="https://github.com/Theano/Theano/blob/master/theano/gpuarray/ctc.py#L20">theano.gpuarray.GpuConnectionistTemporalClassification</a>: COp class that implements the computation of the <span class="caps">CTC</span> loss function on the <span class="caps">GPU</span>.</li>
</ul>
<p>In the COp classes, one can find the paths to the C wrappers, which make the interface between Theano and the warp-ctc&nbsp;library.</p>
<h3>Spatial&nbsp;Transformer</h3>
<p>In the second and third parts of the project, I have worked on implementing a spatial transformer, at first only on the <span class="caps">GPU</span>, and then on the <span class="caps">CPU</span>. A Spatial Transformer is a component of a neural network that can provide spatial manipulation of data within the network. Spatial manipulation can improve models by introducing invariance to affine transformations, such as translation, scaling and rotation. This kind of invariance improves classification performance, since the networks become able to recognize samples that have distortions or are rotated, for&nbsp;example.</p>
<p><img alt="Spatial Transformer Representation" src="spatial_transformer.png" title="Spatial Transformer Representation"></p>
<p>There are three main components in a Spatial Transformer, as shown in the image above (provided in the paper by <a href="https://arxiv.org/abs/1506.02025">Jaderberg et. al</a>):</p>
<ul>
<li><strong>Localisation network:</strong> neural network that receives the input feature map U, where U is a space spanned by the width, height and channels. It outputs the parameters of the transformation to be applied to the feature map. In 2D, the parameters take the form of a 2x3 matrix (i.e. an affine transformation matrix). It can take the form of any neural network, but it should include a final regression layer to produce the transformation&nbsp;parameters.</li>
<li><strong>Grid generator:</strong> normalized grid of coordinates over the input feature map. It maps the original coordinate system of the input to an interval in [-1, 1], and applies the transformation the normalized&nbsp;space.</li>
<li><strong>Sampler:</strong> the sampler takes a set of sampling points from the grid generator, along with the input feature map U and produces the sampled output feature map&nbsp;V.</li>
</ul>
<h4>Spatial Transformer using&nbsp;cuDNN</h4>
<p>cuDNN provides spatial transformer functions since version 6, and those functions were utilized to implement the second part of the project. There are two types of functions: forward and backward. Forward functions implement the operations of the sampling grid, and the sampler. Backward functions are used to compute gradients of the outputs of each forward function, i.e. there is no function to compute gradients of the inputs and another to compute the gradients of the affine transformation, such that they backpropagated in the neural network, in order for it to&nbsp;learn.</p>
<p>Spatial transformer functions from cuDNN were implemented in Theano&#8217;s <code>gpuarray.dnn</code> module, as <a href="http://deeplearning.net/software/theano/extending/op.html">Theano Ops</a>. In order to wrap the required functions, I have implemented wrappers in C, which interface with Theano <code>PyGpuArrayObject</code><span class="quo">&#8216;</span>s and the cuDNN&nbsp;functions.</p>
<p>Below there is a brief description of each Op, and links to where they are located in Theano&#8217;s&nbsp;codebase:</p>
<ul>
<li><a href="https://github.com/Theano/Theano/blob/master/theano/gpuarray/dnn.py#L2973"><code>theano.gpuarray.dnn.dnn_spatialtf</code></a>: function that setups a complete spatial transformer, i.e. it setups the sampling grid and the sampler, and returns the latter to the&nbsp;user.</li>
<li><a href="https://github.com/Theano/Theano/blob/master/theano/gpuarray/dnn.py#L2792"><code>theano.gpuarray.dnn.GpuDnnTransformerGrid</code></a>: COp class that implements the sampling grid, using cuDNN&#8217;s forward grid generator&nbsp;function.</li>
<li><a href="https://github.com/Theano/Theano/blob/master/theano/gpuarray/dnn.py#L2848"><code>theano.gpuarray.dnn.GpuDnnTransformerSampler</code></a>: COp class that implements the sampler, which is currently limited by cuDNN to bilinear interpolation. This class interfaces with cuDNN&#8217;s forward sampler&nbsp;function.</li>
<li><a href="https://github.com/Theano/Theano/blob/master/theano/gpuarray/dnn.py#L2907"><code>theano.gpuarray.dnn.GpuDnnTransformerGradI</code></a>: COp class that implements the gradients of the inputs, which interfaces with cuDNN&#8217;s backward sampling grid&nbsp;function.</li>
<li><a href="https://github.com/Theano/Theano/blob/master/theano/gpuarray/dnn.py#L2944"><code>theano.gpuarray.dnn.GpuDnnTransformerGradT</code></a>: COp class that implements the gradients of the affine transformation, which interfaces with cuDNN&#8217;s backward sampler&nbsp;function.</li>
</ul>
<p>In the COp classes, one can find the paths to the C wrappers, which make the interface between Theano and&nbsp;cuDNN.</p>
<h4>Spatial Transformer on the <span class="caps">CPU</span></h4>
<p>Based on a implementation in <a href="https://github.com/Lasagne/Lasagne/blob/master/lasagne/layers/special.py#L354">Lasagne</a>, a spatial transformer was implemented on the <span class="caps">CPU</span> as well. The work on this third of the project consisted in adapting the implementation from Lasagne, which uses Theano symbolic variables to perform the computations, into Theano&nbsp;Ops.</p>
<p>However, Lasagne does not provide the implementations for the gradients, and neither does cuDNN. So those have to be implemented based on the equations of the paper by Jaderberg et al. [3]. Furthemore, it is necessary to provide a concrete implementation (e.g. using NumPy) of each of those Ops, in order to enable users to debug code that uses the functionalities provided by the spatial&nbsp;transformer.</p>
<p>Most of the implementation is completed, including the gradients of inputs, with only the gradients of the affine transformation currently not passing the gradient tests. Fixing the computations of the affine transformation is the last step required to finish the implementation of the spatial transformer on the <span class="caps">CPU</span>.</p>
<h3>Pull&nbsp;Requests</h3>
<p>A vast majority of the discussions with the mentors during the course of the project, were carried out in the public mailing list of Theano developers, and on GitHub Pull Requests. Furthermore, all implementations passed through a process of unit testing, and peer review by the&nbsp;mentors.</p>
<p>First and second parts of the project were successfully completed, and are already merged into Theano (see the Pull Requests below). However, the third part is not yet completed, for reasons that have already been&nbsp;explained.</p>
<p>Links to the original Pull Requests are provided&nbsp;below:</p>
<ol>
<li>Connectionist Temporal Classification Loss with warp-ctc: <a href="https://github.com/Theano/Theano/pull/5949">Pull&nbsp;Request</a></li>
<li>Spatial Transformer using cuDNN: <a href="https://github.com/Theano/Theano/pull/6061">Pull&nbsp;Request</a></li>
<li>Spatial Transformer on the <span class="caps">CPU</span> (<span class="caps">WIP</span>): <a href="https://github.com/Theano/Theano/pull/6298">Pull&nbsp;Request</a></li>
</ol>
<p>You can also see my commits in Theano, <a href="https://github.com/Theano/Theano/commits/master?author=joaovictortr">here</a> for the <span class="caps">CTC</span> and Spatial Transformer with cuDNN, and <a href="https://github.com/joaovictortr/Theano/commits/spatialtf_cpu?author=joaovictortr">here</a> for the Spatial Transformer on the <span class="caps">CPU</span>.</p>
<h2>Conclusion</h2>
<p>In this project, I have implemented wrappers for <span class="caps">GPU</span> functions in Theano, in order to accelerate the computation of deep learning models. Two of the three parts of the project have been merged into Theano, with the third only requiring fixing the computation of&nbsp;gradients.</p>
<p>During this summer, I have learned a lot about the inner workings of Theano. I have also improved considerably my knowledge of Python, as I come from a strong C/C++&nbsp;background.</p>
<h3>What&#8217;s&nbsp;Next?</h3>
<p>I&#8217;ll start getting deeper into machine learning, and Theano will be a great tool for the job. With some knowledge of the internals, I can implement my own models, as well as suggest and add new&nbsp;functionalities.</p>
<h2>Acknowledgements</h2>
<p>I would like to thank Steven Bocco, my mentor, for guiding me in the execution of the project, providing feedback and reviewing the code. I would also like to thank Frédéric Bastien and Arnaud Bergeron, for helping with organizational aspects, and code&nbsp;reviewing.</p>
<p>Finally, I would like to thank the staff of GSoC, and Google, for this unique&nbsp;opportunity.</p>
<h2>References</h2>
<p>[1] <a href="https://insidehpc.com/2016/01/warp-ctc/">&#8220;Accelerating Machine Learning with Open Source Warp-<span class="caps">CTC</span>&#8221;</a>. 2016. Accessed on:&nbsp;2017-08-28.</p>
<p>[2] Amodei et. al. <a href="https://arxiv.org/abs/1512.02595">Deep Speech 2: End-to-End Speech Recognition in English and Mandarin</a>. 2015. Accessed on:&nbsp;2017-08-28</p>
<p>[3] Jaderberg et. al. <a href="https://arxiv.org/abs/1506.02025">Spatial Transformer Networks</a>. 2015. Accessed on:&nbsp;2017-08-28</p>
<p>[4] Risso, <span class="caps">J. V.</span> T.. <a href="https://www.sharelatex.com/project/58cee4bcf98f21f60fedec74">Extend usage of optimized <span class="caps">GPU</span> libraries in Theano</a>. 2017. Accessed on:&nbsp;2017-08-28</p>
            <a href="#" class="go-top">Go Top</a>
    <div class="comments">
        <div id="disqus_thread"></div>
        <script type="text/javascript">
            /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
            var disqus_shortname = "joaovictortrme"; // required: replace example with your forum shortname

            /* * * DON'T EDIT BELOW THIS LINE * * */
            (function() {
                var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
                dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
                (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
            })();
        </script>
        <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
        <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
    </div>
<footer class="footer">
    <p>&copy; João Victor Risso &ndash;
        Built with <a href="https://github.com/PurePelicanTheme/pure-single">Pure Theme</a>
        for <a href="http://blog.getpelican.com/">Pelican</a>
    </p>
</footer>        </div>
    </div>
    </div>
    <script>
        var $top = $('.go-top');

        // Show or hide the sticky footer button
        $(window).scroll(function() {
            if ($(this).scrollTop() > 200) {
                $top.fadeIn(200);
            } else {
                $top.fadeOut(200);
            }
        });

        // Animate the scroll to top
        $top.click(function(event) {
            event.preventDefault();
            $('html, body').animate({scrollTop: 0}, 300);
        })

        // Makes sure that the href="#" attached to the <a> elements
        // don't scroll you back up the page.
        $('body').on('click', 'a[href="#"]', function(event) {
            event.preventDefault();
        });
    </script>

</body>
</html>